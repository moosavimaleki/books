
In the lambda approach, the stream processor consumes the events and quickly produces an approximate
update to the view; the batch processor later consumes the same set of events and produces a
corrected version of the derived view. The reasoning behind this design is that batch processing is
simpler and thus less prone to bugs, while stream processors are thought to be less reliable and
harder to make fault-tolerant (see [“Fault Tolerance”](ch11.html#sec_stream_fault_tolerance)). Moreover, the stream process can
use fast approximate algorithms while the batch process uses slower exact algorithms. The lambda architecture was an influential idea that shaped the design of data systems for the
better, particularly by popularizing the principle of deriving views onto streams of immutable
events and reprocessing events when needed. However, I also think that it has a number of
practical problems: *  
Having to maintain the same logic to run both in a batch and in a stream processing framework is
significant additional effort. Although libraries such as Summingbird
[[13](ch12.html#Boykin2014vf)] provide an abstraction for
computations that can be run in either a batch or a streaming context, the operational complexity
of debugging, tuning, and maintaining two different systems remains
[[14](ch12.html#Kreps2014wv_ch12)].