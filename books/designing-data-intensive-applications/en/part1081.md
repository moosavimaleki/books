The most common way of achieving this consensus is to make a single node the leader, and put it in
charge of making all the decisions. That works fine as long as you don’t mind funneling all requests
through a single node (even if the client is on the other side of the world), and as long as that
node doesn’t fail. If you need to tolerate the leader failing, you’re back at the consensus problem
again (see [“Single-leader replication and consensus”](ch09.html#sec_consistency_consensus_leader)). Uniqueness checking can be scaled out by partitioning based on the value that needs to be unique.
For example, if you need to ensure uniqueness by request ID, as in [Example 12-2](#fig_future_request_id), you
can ensure all requests with the same request ID are routed to the same partition (see
[Chapter 6](ch06.html#ch_partitioning)). If you need usernames to be unique, you can partition by hash of username. However, asynchronous multi-master replication is ruled out, because it could happen that different
masters concurrently accept conflicting writes, and thus the values are no longer unique (see
[“Implementing Linearizable Systems”](ch09.html#sec_consistency_implementing_linearizable)). If you want to be able to immediately reject any
writes that would violate the constraint, synchronous coordination is unavoidable
[[56](ch12.html#Bailis2014th_ch12)].