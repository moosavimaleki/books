These assumptions are quite reasonable, as they are true most of the time, and it would be difficult
to get anything done if we had to constantly worry about our computers making mistakes.
Traditionally, system models take a binary approach toward faults: we assume that some things can
happen, and other things can never happen. In reality, it is more a question of probabilities: some
things are more likely, other things less likely. The question is whether violations of our
assumptions happen often enough that we may encounter them in practice. 
We have seen that data can become corrupted while it is sitting untouched on disks (see
[“Replication and Durability”](ch07.html#sidebar_transactions_durability)), and data corruption on the network can sometimes evade the TCP
checksums (see [“Weak forms of lying”](ch08.html#sec_distributed_weak_lying)). Maybe this is something we should be paying more
attention to? 
One application that I worked on in the past collected crash reports from clients, and some of the
reports we received could only be explained by random bit-flips in the memory of those devices. It
seems unlikely, but if you have enough devices running your software, even very unlikely things do
happen.