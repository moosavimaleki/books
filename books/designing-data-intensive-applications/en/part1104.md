
By contrast, event-based systems can provide better auditability. In the event sourcing approach,
user input to the system is represented as a single immutable event, and any resulting state updates
are derived from that event. The derivation can be made deterministic and repeatable, so that
running the same log of events through the same version of the derivation code will result in the
same state updates. 
Being explicit about dataflow (see [“Philosophy of batch process outputs”](ch10.html#sec_batch_philosophy)) makes the provenance of data much
clearer, which makes integrity checking much more feasible. For the event log, we can use hashes to
check that the event storage has not been corrupted. For any derived state, we can rerun the batch
and stream processors that derived it from the event log in order to check whether we get the same
result, or even run a redundant derivation in parallel. A deterministic and well-defined dataflow also makes it easier to debug and trace the execution of a
system in order to determine why it did something
[[4](ch12.html#Kerr2016va),
[69](ch12.html#Fowler2011wp_ch12)]. If something unexpected occurred, it is valuable
to have the diagnostic capability to reproduce the exact circumstances that led to the unexpected
event—a kind of time-travel debugging capability.