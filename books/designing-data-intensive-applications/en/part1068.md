
For example, Kyle Kingsbury’s Jepsen experiments
[[53](ch12.html#KingsburyJepsen)]
have highlighted the stark discrepancies between some products’ claimed safety guarantees and their
actual behavior in the presence of network problems and crashes. Even if infrastructure products
like databases were free from problems, application code would still need to correctly use the
features they provide, which is error-prone if the configuration is hard to understand (which is the
case with weak isolation levels, quorum configurations, and so on). If your application can tolerate occasionally corrupting or losing data in unpredictable ways, life
is a lot simpler, and you might be able to get away with simply crossing your fingers and hoping for
the best. On the other hand, if you need stronger assurances of correctness, then serializability and
atomic commit are established approaches, but they come at a cost: they typically only work in a
single datacenter (ruling out geographically distributed architectures), and they limit the scale
and fault-tolerance properties you can achieve.