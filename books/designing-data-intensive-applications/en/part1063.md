
We discussed that when a stream processor writes derived data to a store (database, cache, or index),
and when user requests query that store, the store acts as the boundary between the write path
and the read path. The store allows random-access read queries to the data that would otherwise
require scanning the whole event log. In many cases, the data storage is separate from the streaming system. But recall that stream
processors also need to maintain state to perform aggregations and joins (see [“Stream Joins”](ch11.html#sec_stream_joins)).
This state is normally hidden inside the stream processor, but some frameworks allow it to also be
queried by outside clients
[[45](ch12.html#Thereska2016ul)],
turning the stream processor itself into a kind of simple database. I would like to take that idea further. As discussed so far, the writes to the store go through an
event log, while reads are transient network requests that go directly to the nodes that store the
data being queried. This is a reasonable design, but not the only possible one. It is also possible
to represent read requests as streams of events, and send both the read events and the write events
through a stream processor; the processor responds to read events by emitting the result of the read
to an output stream [[46](ch12.html#McSherry2016vk)].