Whenever you run CREATE INDEX, the database essentially reprocesses the existing dataset (as
discussed in [“Reprocessing data for application evolution”](#sec_future_reprocessing)) and derives the index as a new view onto the existing
data. The existing data may be a snapshot of the state rather than a log of all changes that ever
happened, but the two are closely related (see [“State, Streams, and Immutability”](ch11.html#sec_stream_immutability)). ### The meta-database of everything In this light, I think that the dataflow across an entire organization starts looking like one huge
database [[7](ch12.html#Kreps2013vs_ch12)]. Whenever a batch, stream,
or ETL process transports data from one place and form to another place and form, it is acting like
the database subsystem that keeps indexes or materialized views up to date. Viewed like this, batch and stream processors are like elaborate implementations of triggers, stored
procedures, and materialized view maintenance routines. The derived data systems they maintain are
like different index types. For example, a relational database may support B-tree indexes, hash
indexes, spatial indexes (see [“Multi-column indexes”](ch03.html#sec_storage_index_multicolumn)), and other types of indexes. In the
emerging architecture of derived data systems, instead of implementing those facilities as features
of a single integrated database product, they are provided by various different pieces of software,
running on different machines, administered by different teams.