
In many countries, anti-discrimination laws prohibit treating people differently depending on
protected traits such as ethnicity, age, gender, sexuality, disability, or beliefs. Other features
of a person’s data may be analyzed, but what happens if they are correlated with protected traits?
For example, in racially segregated neighborhoods, a person’s postal code or even their IP address
is a strong predictor of race. Put like this, it seems ridiculous to believe that an algorithm could
somehow take biased data as input and produce fair and impartial output from it
[[85](ch12.html#Emspak2016no)]. Yet this belief
often seems to be implied by proponents of data-driven decision making, an attitude that has been
satirized as “machine learning is like money laundering for bias”
[[86](ch12.html#Ceglowski2016uw)]. Predictive analytics systems merely extrapolate from the past; if the past is discriminatory, they
codify that discrimination. If we want the future to be better than the past, moral imagination is
required, and that’s something only humans can provide
[[87](ch12.html#ONeil2016vh)]. Data and models should be our tools, not our masters.