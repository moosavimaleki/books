در بسیاری از کشورها، قوانین ضد تبعیض، رفتار متفاوت با افراد بر اساس ویژگی‌های محافظت‌شده مانند قومیت، سن، جنسیت، گرایش جنسی، معلولیت یا عقاید را ممنوع می‌کند. سایر ویژگی‌های داده‌های یک فرد ممکن است تحلیل شوند، اما اگر آنها با ویژگی‌های محافظت‌شده همبستگی داشته باشند چه اتفاقی می‌افتد؟ به عنوان مثال، در محله‌های جدا شده نژادی، کد پستی یا حتی آدرس IP یک فرد پیش‌بینی‌کننده قوی نژاد است. با این توضیح، به نظر مسخره می‌رسد که باور کنیم یک الگوریتم می‌تواند به نوعی داده‌های متعصبانه را به عنوان ورودی دریافت کند و از آن خروجی عادلانه و بی‌طرفانه تولید کند [[85](ch12.html#Emspak2016no)].

با این حال، این باور اغلب به نظر می‌رسد توسط طرفداران تصمیم‌گیری مبتنی بر داده ضمنی باشد، نگرشی که به عنوان "یادگیری ماشینی مانند پولشویی برای تعصب است" به طنز گرفته شده است [[86](ch12.html#Ceglowski2016uw)].

سیستم‌های تحلیل پیش‌بینی‌کننده صرفاً از گذشته برون‌یابی می‌کنند؛ اگر گذشته تبعیض‌آمیز باشد، آنها آن تبعیض را کدگذاری می‌کنند. اگر می‌خواهیم آینده بهتر از گذشته باشد، تخیل اخلاقی لازم است، و این چیزی است که فقط انسان‌ها می‌توانند ارائه دهند [[87](ch12.html#ONeil2016vh)]. داده‌ها و مدل‌ها باید ابزارهای ما باشند، نه اربابان ما.