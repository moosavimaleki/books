A consequence of the dynamic allocation and deallocation in response to individual requests is that these short-lived containers cannot maintain any state: The containers must be stateless. In a serverless architecture, any state needed for coordination must be stored in an infrastructure service delivered by the cloud provider or passed as parameters. Cloud providers impose some practical limitations on FaaS features. The first is that the providers have a limited selection of base container images, which restricts your programming language options and library dependencies. This is done to reduce the container load time—your service is constrained to be a thin image layer on top of the provider’s base image layer. The next limitation is that the “cold start” time, when your container is allocated and loaded the first time, can be several seconds. Subsequent requests are handled nearly instantaneously, as your container image is cached on a node. Finally, the execution time for a request is limited—your service must process the request and exit within the provider’s time limit or it will be terminated. Cloud providers do this for economic reasons, so that they can tailor the pricing of FaaS compared to other ways of running containers, and to ensure that no FaaS user consumes too much of the resource pool. Some designers of serverless systems devote considerable energy to working around or defeating these limitations—for example, prestarting services to avoid cold-start latency, making dummy requests to keep services in cache, and forking or chaining requests from one service to another to extend the effective execution time.